{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Fall 2021 Assignment 4 \n",
    "### Author: George Tzanetakis \n",
    "\n",
    "This notebook is based on the topics covered in **Chapter 14 - Probabilistic Reasoning over Time**, **Chapter 20 - Learning probabilistic models**, and **Chapter 19 Learning from Examples** from the book *Artificial Intelligence: A Modern Approach.*  You are welcome and actually it can be educational to look at the code at the aima-code repository as well as other code resources you can find on the web. However, make sure you understand any code that you incoporate. \n",
    "\n",
    "The assignment structure is as follows - each item is worth 1 point: \n",
    "\n",
    "1. Bayesian Network  (Basic) - express network and print CPT  \n",
    "2. Bayesian Network  (Expected) - markdown and direct inference   \n",
    "3. Bayesian Network  (Basic) -  approximate inference (rejection sampling and likelihood weighting) \n",
    "4. Bayesian Netowrk  (Advanced) - naive bayes of movie reviews as bayesian network \n",
    "5. Hidden Markov Models (Basic) - Use HMM to generate plausible DNA sequences and visualize \n",
    "6. Hidden Markov Models (Expected) - Learn HMM from samples for DNA sequences \n",
    "7. Hidden Markov Model (Expected) - Compare classification accuracy of ignoring transition matrix \n",
    "8. Hidden Markov Models (Advanced) - make up HMM scenario for activity detection using 2D coordinates and GMMs  \n",
    "9. Classification (Basic) - Replicate movie review classification using bernoulli Naive Bayes in sklearn \n",
    "10. Classification(Expected) - Explore a standard classification problem with continuous attributes in sklearn \n",
    "\n",
    "The grading will be done in 0.5 increments. 1 point for correct answer, 0.5 points for partial or incorrect \n",
    "but reasonable answer and 0.0 for no answer or completely wrong answer. \n",
    "\n",
    "**Misunderstanding of probability may be the greatest of all impediments\n",
    "to scientific literacy.** \n",
    "\n",
    "**Gould, Stephen Jay** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (Basic)  - 1 point\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"dispnea.png\">\n",
    "\n",
    "Using the convetions for DBNs used in probability.ipynb (from the AIMA authors) encode the diapnea network shown above. Once you have constructed the Bayesian network display the cpt for the Lung Cancer Node (using the API provided not just showing the numbers).\n",
    "\n",
    "The cell below contains the code that defined BayesNodes and BayesNetworks and the following cell \n",
    "shows an example of defining the Burglary network and performing a query using direct enumeration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.21.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.21.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (6.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.28.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (1.2.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (49.6.0.post20210108)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.9/site-packages (from scipy) (1.21.4)\n",
      "Requirement already satisfied: hmmlearn in /opt/conda/lib/python3.9/site-packages (0.2.6)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.21.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install hmmlearn\n",
    "!pip install sklearn\n",
    "import numpy as np \n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from hmmlearn import hmm\n",
    "\n",
    "\n",
    "def extend(s, var, val):\n",
    "    \"\"\"Copy dict s and extend it by setting var to val; return copy.\"\"\"\n",
    "    return {**s, var: val}\n",
    "\n",
    "def event_values(event, variables):                                                                      \n",
    "    \"\"\"Return a tuple of the values of variables in event.                                               \n",
    "    >>> event_values ({'A': 10, 'B': 9, 'C': 8}, ['C', 'A'])                                             \n",
    "    (8, 10)                                                                                              \n",
    "    >>> event_values ((1, 2), ['C', 'A'])                                                                \n",
    "    (1, 2)                                                                                               \n",
    "    \"\"\"                                                                                                  \n",
    "    if isinstance(event, tuple) and len(event) == len(variables):                                        \n",
    "        return event                                                                                     \n",
    "    else:                                                                                                \n",
    "        return tuple([event[var] for var in variables])                                                  \n",
    "                      \n",
    "def probability(p):                                                                                      \n",
    "    \"\"\"Return true with probability p.\"\"\"                                                                \n",
    "    return p > random.uniform(0.0, 1.0)  \n",
    "        \n",
    "class ProbDist:\n",
    "    \"\"\"A discrete probability distribution. You name the random variable\n",
    "    in the constructor, then assign and query probability of values.\n",
    "    >>> P = ProbDist('Flip'); P['H'], P['T'] = 0.25, 0.75; P['H']\n",
    "    0.25\n",
    "    >>> P = ProbDist('X', {'lo': 125, 'med': 375, 'hi': 500})\n",
    "    >>> P['lo'], P['med'], P['hi']\n",
    "    (0.125, 0.375, 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var_name='?', freq=None):\n",
    "        \"\"\"If freq is given, it is a dictionary of values - frequency pairs,\n",
    "        then ProbDist is normalized.\"\"\"\n",
    "        self.prob = {}\n",
    "        self.var_name = var_name\n",
    "        self.values = []\n",
    "        if freq:\n",
    "            for (v, p) in freq.items():\n",
    "                self[v] = p\n",
    "            self.normalize()\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        \"\"\"Given a value, return P(value).\"\"\"\n",
    "        try:\n",
    "            return self.prob[val]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def __setitem__(self, val, p):\n",
    "        \"\"\"Set P(val) = p.\"\"\"\n",
    "        if val not in self.values:\n",
    "            self.values.append(val)\n",
    "        self.prob[val] = p\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Make sure the probabilities of all values sum to 1.\n",
    "        Returns the normalized distribution.\n",
    "        Raises a ZeroDivisionError if the sum of the values is 0.\"\"\"\n",
    "        total = sum(self.prob.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            for val in self.prob:\n",
    "                self.prob[val] /= total\n",
    "        return self\n",
    "\n",
    "    def show_approx(self, numfmt='{:.3g}'):\n",
    "        \"\"\"Show the probabilities rounded and sorted by key, for the\n",
    "        sake of portable doctests.\"\"\"\n",
    "        return ', '.join([('{}: ' + numfmt).format(v, p) for (v, p) in sorted(self.prob.items())])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"P({})\".format(self.var_name)\n",
    "\n",
    "\n",
    "class BayesNode:\n",
    "    \"\"\"A conditional probability distribution for a boolean variable,\n",
    "    P(X | parents). Part of a BayesNet.\"\"\"\n",
    "\n",
    "    def __init__(self, X, parents, cpt):\n",
    "        \"\"\"X is a variable name, and parents a sequence of variable\n",
    "        names or a space-separated string. cpt, the conditional\n",
    "        probability table, takes one of these forms:\n",
    "\n",
    "        * A number, the unconditional probability P(X=true). You can\n",
    "          use this form when there are no parents.\n",
    "\n",
    "        * A dict {v: p, ...}, the conditional probability distribution\n",
    "          P(X=true | parent=v) = p. When there's just one parent.\n",
    "\n",
    "        * A dict {(v1, v2, ...): p, ...}, the distribution P(X=true |\n",
    "          parent1=v1, parent2=v2, ...) = p. Each key must have as many\n",
    "          values as there are parents. You can use this form always;\n",
    "          the first two are just conveniences.\n",
    "\n",
    "        In all cases the probability of X being false is left implicit,\n",
    "        since it follows from P(X=true).\n",
    "\n",
    "        >>> X = BayesNode('X', '', 0.2)\n",
    "        >>> Y = BayesNode('Y', 'P', {T: 0.2, F: 0.7})\n",
    "        >>> Z = BayesNode('Z', 'P Q',\n",
    "        ...    {(T, T): 0.2, (T, F): 0.3, (F, T): 0.5, (F, F): 0.7})\n",
    "        \"\"\"\n",
    "        if isinstance(parents, str):\n",
    "            parents = parents.split()\n",
    "\n",
    "        # We store the table always in the third form above.\n",
    "        if isinstance(cpt, (float, int)):  # no parents, 0-tuple\n",
    "            cpt = {(): cpt}\n",
    "        elif isinstance(cpt, dict):\n",
    "            # one parent, 1-tuple\n",
    "            if cpt and isinstance(list(cpt.keys())[0], bool):\n",
    "                cpt = {(v,): p for v, p in cpt.items()}\n",
    "\n",
    "        assert isinstance(cpt, dict)\n",
    "        for vs, p in cpt.items():\n",
    "            assert isinstance(vs, tuple) and len(vs) == len(parents)\n",
    "            assert all(isinstance(v, bool) for v in vs)\n",
    "            assert 0 <= p <= 1\n",
    "\n",
    "        self.variable = X\n",
    "        self.parents = parents\n",
    "        self.cpt = cpt\n",
    "        self.children = []\n",
    "\n",
    "    def p(self, value, event):\n",
    "        \"\"\"Return the conditional probability\n",
    "        P(X=value | parents=parent_values), where parent_values\n",
    "        are the values of parents in event. (event must assign each\n",
    "        parent a value.)\n",
    "        >>> bn = BayesNode('X', 'Burglary', {T: 0.2, F: 0.625})\n",
    "        >>> bn.p(False, {'Burglary': False, 'Earthquake': True})\n",
    "        0.375\"\"\"\n",
    "        assert isinstance(value, bool)\n",
    "        ptrue = self.cpt[event_values(event, self.parents)]\n",
    "        return ptrue if value else 1 - ptrue\n",
    "\n",
    "    def sample(self, event):\n",
    "        \"\"\"Sample from the distribution for this variable conditioned\n",
    "        on event's values for parent_variables. That is, return True/False\n",
    "        at random according with the conditional probability given the\n",
    "        parents.\"\"\"\n",
    "        return probability(self.p(True, event))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.variable, ' '.join(self.parents)))\n",
    "    \n",
    "    \n",
    "class BayesNet:\n",
    "    \"\"\"Bayesian network containing only boolean-variable nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, node_specs=None):\n",
    "        \"\"\"Nodes must be ordered with parents before children.\"\"\"\n",
    "        self.nodes = []\n",
    "        self.variables = []\n",
    "        node_specs = node_specs or []\n",
    "        for node_spec in node_specs:\n",
    "            self.add(node_spec)\n",
    "\n",
    "    def add(self, node_spec):\n",
    "        \"\"\"Add a node to the net. Its parents must already be in the\n",
    "        net, and its variable must not.\"\"\"\n",
    "        node = BayesNode(*node_spec)\n",
    "        assert node.variable not in self.variables\n",
    "        assert all((parent in self.variables) for parent in node.parents)\n",
    "        self.nodes.append(node)\n",
    "        self.variables.append(node.variable)\n",
    "        for parent in node.parents:\n",
    "            self.variable_node(parent).children.append(node)\n",
    "\n",
    "    def variable_node(self, var):\n",
    "        \"\"\"Return the node for the variable named var.\n",
    "        >>> burglary.variable_node('Burglary').variable\n",
    "        'Burglary'\"\"\"\n",
    "        for n in self.nodes:\n",
    "            if n.variable == var:\n",
    "                return n\n",
    "        raise Exception(\"No such variable: {}\".format(var))\n",
    "\n",
    "    def variable_values(self, var):\n",
    "        \"\"\"Return the domain of var.\"\"\"\n",
    "        return [True, False]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BayesNet({0!r})'.format(self.nodes)\n",
    "    \n",
    "    \n",
    "def enumerate_all(variables, e, bn):\n",
    "    \"\"\"Return the sum of those entries in P(variables | e{others})\n",
    "    consistent with e, where P is the joint distribution represented\n",
    "    by bn, and e{others} means e restricted to bn's other variables\n",
    "    (the ones other than variables). Parents must precede children in variables.\"\"\"\n",
    "    if not variables:\n",
    "        return 1.0\n",
    "    Y, rest = variables[0], variables[1:]\n",
    "    Ynode = bn.variable_node(Y)\n",
    "    if Y in e:\n",
    "        return Ynode.p(e[Y], e) * enumerate_all(rest, e, bn)\n",
    "    else:\n",
    "        return sum(Ynode.p(y, e) * enumerate_all(rest, extend(e, Y, y), bn)\n",
    "                   for y in bn.variable_values(Y))\n",
    "\n",
    "def enumeration_ask(X, e, bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.9]\n",
    "    Return the conditional probability distribution of variable X\n",
    "    given evidence e, from BayesNet bn.\n",
    "    >>> enumeration_ask('Burglary', dict(JohnCalls=T, MaryCalls=T), burglary\n",
    "    ...  ).show_approx()\n",
    "    'False: 0.716, True: 0.284'\"\"\"\n",
    "    assert X not in e, \"Query variable must be distinct from evidence\"\n",
    "    Q = ProbDist(X)\n",
    "    for xi in bn.variable_values(X):\n",
    "        Q[xi] = enumerate_all(bn.variables, extend(e, X, xi), bn)\n",
    "    return Q.normalize()\n",
    "\n",
    "def consistent_with(event, evidence):\n",
    "    \"\"\"Is event consistent with the given evidence?\"\"\"\n",
    "    return all(evidence.get(k, v) == v for k, v in event.items())\n",
    "\n",
    "def prior_sample(bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.13]\n",
    "    Randomly sample from bn's full joint distribution.\n",
    "    The result is a {variable: value} dict.\n",
    "    \"\"\"\n",
    "    event = {}\n",
    "    for node in bn.nodes:\n",
    "        event[node.variable] = node.sample(event)\n",
    "    return event\n",
    "\n",
    "def rejection_sampling(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.14]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn, using N samples.\n",
    "    Raises a ZeroDivisionError if all the N samples are rejected,\n",
    "    i.e., inconsistent with e.\n",
    "    >>> random.seed(47)\n",
    "    >>> rejection_sampling('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.7, True: 0.3'\n",
    "    \"\"\"\n",
    "    counts = {x: 0 for x in bn.variable_values(X)}  # bold N in [Figure 14.14]\n",
    "    for j in range(N):\n",
    "        sample = prior_sample(bn)  # boldface x in [Figure 14.14]\n",
    "        if consistent_with(sample, e):\n",
    "            counts[sample[X]] += 1\n",
    "    return ProbDist(X, counts)\n",
    "\n",
    "def weighted_sample(bn, e):\n",
    "    \"\"\"\n",
    "    Sample an event from bn that's consistent with the evidence e;\n",
    "    return the event and its weight, the likelihood that the event\n",
    "    accords to the evidence.\n",
    "    \"\"\"\n",
    "    w = 1\n",
    "    event = dict(e)  # boldface x in [Figure 14.15]\n",
    "    for node in bn.nodes:\n",
    "        Xi = node.variable\n",
    "        if Xi in e:\n",
    "            w *= node.p(e[Xi], event)\n",
    "        else:\n",
    "            event[Xi] = node.sample(event)\n",
    "    return event, w\n",
    "\n",
    "def likelihood_weighting(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.15]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn.\n",
    "    >>> random.seed(1017)\n",
    "    >>> likelihood_weighting('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.702, True: 0.298'\n",
    "    \"\"\"\n",
    "    W = {x: 0 for x in bn.variable_values(X)}\n",
    "    for j in range(N):\n",
    "        sample, weight = weighted_sample(bn, e)  # boldface x, w in [Figure 14.15]\n",
    "        W[sample[X]] += weight\n",
    "    return ProbDist(X, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}\n",
      "0.2841718353643929 0.7158281646356071\n",
      "0.16666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False: 0.785, True: 0.215'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   burglary = BayesNet([\n",
    "        ('Burglary', '', 0.001),\n",
    "        ('Earthquake', '', 0.002),\n",
    "        ('Alarm', 'Burglary Earthquake',\n",
    "         {(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}),\n",
    "        ('JohnCalls', 'Alarm', {True: 0.90, False: 0.05}),\n",
    "        ('MaryCalls', 'Alarm', {True: 0.70, False: 0.01})\n",
    "    ])\n",
    "    \n",
    "print(burglary.variable_node('Alarm').cpt)\n",
    "ans_dist = enumeration_ask('Burglary', {'JohnCalls': True, 'MaryCalls': True}, burglary)\n",
    "print(ans_dist[True],ans_dist[False])\n",
    "\n",
    "\n",
    "p = rejection_sampling('Burglary', dict(JohnCalls=True, MaryCalls=True), burglary, 10000)\n",
    "print(p[True])\n",
    "\n",
    "likelihood_weighting('Burglary', dict(JohnCalls=True, MaryCalls=True),burglary, 10000).show_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True,): 0.1, (False,): 0.01}\n"
     ]
    }
   ],
   "source": [
    "smoke = BayesNet([\n",
    "    ('Asia', '', 0.01),\n",
    "    ('Smoker', '', 0.5),\n",
    "    ('Tuberculosis', 'Asia', {True: 0.05, False: 0.01}),\n",
    "    ('LungCancer', 'Smoker', {True: 0.1, False: 0.01}),\n",
    "    ('Bronchitis', 'Smoker', {True: 0.6, False: 0.3}),\n",
    "    ('TB&LungCancer', 'Tuberculosis LungCancer',\n",
    "     {(True, True): 1, (True, False): 1, (False, True): 1, (False, False): 0}),\n",
    "    ('PositiveXRay', 'TB&LungCancer', {True: 0.98, False: 0.05}),\n",
    "    ('Dispnea', 'TB&LungCancer Bronchitis', \n",
    "     {(True, True): 0.9, (True, False): 0.7, (False, True): 0.8, (False, False): 0.1})\n",
    "])\n",
    "\n",
    "print(smoke.variable_node('LungCancer').cpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (Expected) 1 point \n",
    "\n",
    "Answer using exact inference with enumeration the following query: given that a patient has been in Asia and has a positive xray, what is the likelihood of having dispnea?\n",
    "\n",
    "Write down using markdown the expression that corresponds to this query and the corresponding numbers from the CPT. There will be multiple sums and subscripts. Calculate the result using a calculator.\n",
    "\n",
    "Write code for the same query using enumeration_ask and confirm that the result is the same for the same query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6811011940658546\n",
      "0.10000000000000002\n",
      "0.10000000000000003\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#In Markdown\n",
    "# P(Dyspnea|Asia,PositiveXRay)\n",
    "\n",
    "ans = enumeration_ask('Dispnea', {'Asia': True, 'PositiveXRay': True}, smoke)\n",
    "ans1 = enumeration_ask('Dispnea', {'Asia': True, 'PositiveXRay': True, 'Smoker': False, 'Tuberculosis': False, 'LungCancer': False, 'Bronchitis': False, 'TB&LungCancer': False}, smoke)\n",
    "ans2 = enumeration_ask('Dispnea', {'Asia': True, 'PositiveXRay': True, 'Smoker':True, 'Tuberculosis': False, 'LungCancer': False, 'Bronchitis': False, 'TB&LungCancer': False}, smoke)\n",
    "\n",
    "\n",
    "print(ans[True])\n",
    "print(ans1[True])\n",
    "print(ans2[True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3 (Basic) - 1 point\n",
    "\n",
    "Answer using approximate inference the same query using both rejection sampling and likelihood weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False: 0.321, True: 0.679'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "p = rejection_sampling('Dispnea', dict(Asia=True, PositiveXRay=True), smoke, 10000)\n",
    "print(p[True])\n",
    "\n",
    "likelihood_weighting('Dispnea', dict(Asia=True, PositiveXRay=True), smoke, 10000).show_approx()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4 (ADVANCED) - 1 point \n",
    "\n",
    "A Naive Bayes classifier can be considered as a Bayesian Network. The classification problem can then be expressed as setting all the variables corresponding to the features as evidence and querying the probability for the class. Express the Bernoulli Naive Bayes classifier you implemented in the previous assignment as a Bayesian Network using the probability.ipynb conventions used in this notebook. Now that you have a DBN express and solve the classification problem as a query and go over all the previous steps for this particular problem. More specifically do exact inference by enumeration, approximate inference by rejection sampling to answer the query and show the results. Use 4 specific examples (2 positive and 2 negative) from the training dataset to show how the prediction using the Bayesian network works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awful and Bad\n",
      "Postive: 0.07350434750069348\n",
      "Negative: 0.9264956524993065 \n",
      "\n",
      "Great and Enjoyable\n",
      "Postive 0.8148789772161842\n",
      "Negative 0.18512102278381576\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reviews = BayesNet([\n",
    "    ('Positive', '', 0.5),\n",
    "    ('Negative', '', 0.5),\n",
    "    ('Awful', 'Positive Negative', {(True, True): 0.0, (True, False): 0.019, (False, True): 0.101, (False, False): 0.0}),\n",
    "    ('Bad', 'Positive Negative', {(True, True): 0.0, (True, False): 0.257, (False, True): 0.505, (False, False): 0.0}),\n",
    "    ('Boring', 'Positive Negative', {(True, True): 0.0, (True, False): 0.048, (False, True): 0.169, (False, False): 0.0}),\n",
    "    ('Dull', 'Positive Negative', {(True, True): 0.0, (True, False): 0.023, (False, True): 0.091, (False, False): 0.0}),\n",
    "    ('Effective', 'Positive Negative', {(True, True): 0.0, (True, False): 0.120, (False, True): 0.046, (False, False): 0.0}),\n",
    "    ('Enjoyable', 'Positive Negative', {(True, True): 0.0, (True, False): 0.095, (False, True): 0.053, (False, False): 0.0}),\n",
    "    ('Great', 'Positive Negative', {(True, True): 0.0, (True, False): 0.408, (False, True): 0.285, (False, False): 0.0}),\n",
    "    ('Hilarious', 'Positive Negative', {(True, True): 0.0, (True, False): 0.124, (False, True): 0.050, (False, False): 0.0}),\n",
    "])\n",
    "\n",
    "#\"Awful and Bad\"\n",
    "ans = enumeration_ask('Positive', {'Awful': True, 'Bad': True, 'Boring': False, 'Dull': False, 'Effective': False, 'Enjoyable':False, 'Great':False, 'Hilarious': False}, reviews)\n",
    "ans1 = enumeration_ask('Negative', {'Awful': True, 'Bad': True, 'Boring': False, 'Dull': False, 'Effective': False, 'Enjoyable':False, 'Great':False, 'Hilarious': False}, reviews)\n",
    "\n",
    "#\"Great and Enjoyable\"\n",
    "ans3 = enumeration_ask('Positive', {'Awful': False, 'Bad': False, 'Boring': False, 'Dull': False, 'Effective': False, 'Enjoyable':True, 'Great':True, 'Hilarious': False}, reviews)\n",
    "ans4 = enumeration_ask('Negative', {'Awful': False, 'Bad': False, 'Boring': False, 'Dull': False, 'Effective': False, 'Enjoyable':True, 'Great':True, 'Hilarious': False}, reviews)\n",
    "\n",
    "print(\"Awful and Bad\")\n",
    "print(\"Postive:\", ans[True])\n",
    "print(\"Negative:\", ans1[True], '\\n')\n",
    "print(\"Great and Enjoyable\")\n",
    "print(\"Postive\", ans3[True])\n",
    "print(\"Negative\", ans4[True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (Basic) -1 point\n",
    "\n",
    "\n",
    "The next three question explore hidden markov models (HMMs) and use the hmmlearn Python library. You can use the code for the weather example in the probabilistic reasoning over time notebook we covered in class as a template for writing your code. \n",
    "\n",
    "The problem used in inspired by the use of HMMs in bioinformatics. \n",
    "There are several simplifications made to make it reasonable as part of an assignment. DNA sequences can be considered strings over an alphabet of 4 symbols/nucleobases **A,C,T,G (adenine, cytosine, thymine, guanine**. Parts of a DNA sequence are dense with C and G and other parts are sparse with C and G and it is of interest to biologists to identify these regions. \n",
    "\n",
    "We will model the CG-dense **(CGD)** and **CG-sparse** (CGS) as hidden states and the nucleobases are the observations. Through experimental data we have the following information: \n",
    "\n",
    "1. The transition probability from CGD to CGS is 0.37 and the probability of staying in CGD is 0.63. The transition probability from CGS to CGD is similarly 0.37 with 0.63 being the probability of staying in CGS. \n",
    "\n",
    "2. The observation probabilities of CGD regions are: A: 0.15, C:0.35, G: 0.35, and T:0.15. The observation probabilities of CGS regions are: A: 0.40, C: 0.10, G: 0.10, T: 0.40 \n",
    "\n",
    "3. You can assume that the initial state probabilities are the same (0.5) \n",
    "\n",
    "4. For visualization of the DNA sequences use the following color mapping: A: red, C: green, T: blue, G: yellow, and for CGD: black \n",
    "and CGS: white \n",
    "\n",
    "\n",
    "Define this HMM model using the **hmmlearn** conventions. Then use the created model to generate a sequence of 1000 samples (i.e both hidden states and corresponding observations). Use the colors above \n",
    "to visualize the sequence of samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAABlCAYAAABZcXdQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKy0lEQVR4nO3dbaxlV13H8e+PTplKq522U0enbXpLaEomGKSpUBUViylMIZYXTQBNaGqTSiICxkSLvABeaMQQK0aCaaDypC0woDb4UEol8IZWZlCxMK2dqUinTzN9mD4QlZb+fXHWHW9v1z0Pd+655947309ycs5ee9291t5rrzO/Ofs8pKqQJEnSsz1v1h2QJElaiwxJkiRJHYYkSZKkDkOSJElShyFJkiSpw5AkSZLUYUiSJEnqMCRJmpkk703yqQnqvyrJgWn2SZLmGZIkSZI6DEmSVkWS301yb5InktyZ5HXA7wFvTPJkkn9r9a5IsrfVuzvJr7fyE4F/ALa3+k8m2Z7keUmuTrI/ycNJPpPk1PY3JyT5VCs/nOTrSbbN6hhIWl8MSZKmLsl5wNuAn6qqHwZeA9wB/AHw6ao6qape2qofBF4P/AhwBXBNkvOr6nvATuC+Vv+kqroP+E3gDcAvANuBR4EPtW1dDpwMnAWcBrwV+O9p76+kjcGQJGk1/ADYDOxIcnxVfaeq9vcqVtXfVdX+GvgK8EXg54Zs+63Au6vqQFX9L/Be4LIkm4CnGISjF1XVD6pqT1U9vpI7JmnjMiRJmrqq2ge8k0GAOZjkhiTbe3WT7Exya5JHkhwGLgG2Dtn82cBft8tph4G9DELZNuCTwE3ADUnuS/JHSY5fod2StMEZkiStiqr6q6p6JYNQU8D72/0RSTYDnwM+AGyrqi3A3wOZ30xn0/cAO6tqy4LbCVV1b1U9VVXvq6odwM8wuIz3lmnsn6SNx5AkaeqSnJfkohaC/ofB+4KeAR4E5pLMPxc9n8FluUPA00l2Ahcv2NSDwGlJTl5Q9ufA7yc5u7V1epJL2+NfTPITSY4DHmdw+e2Zqe2opA3FkCRpNWwG/hB4CHgA+FHgXcBn2/qHk3yjqp4A3g58hsEbsH8FuHF+I1V1B3A9cHe7vLYd+GCr88UkTwC3Aq9of/JjwC4GAWkv8BUGl+AkaaRU9V69liRJOrb5SpIkSVKHIUmSJKnDkCRJktRhSJIkSerYNKpCkusYfLfIwap6yTgb3bp1a83NzR1l1yRJkqZvz549D1XV6YvLR4Yk4GPAnwGfGLexubk5du/ePX7vJEmSZiTJf/XKR15uq6qvAo+seI8kSZLWsBV7T1KSq5LsTrL70KFDK7XZYe0duV94G2fd/OPFy4vrjmqrV3+SssVt9NYP6/tS2+nd9/rS216vzVHHathxG7Xfo/Z9qX5Psj89S50zS/VnqeM6zjm2VPvL6dOwdnv1lmprqX0Z1sdhc2jY/o06Zku1P872R513o9oZNX7Dzu3e+lHLo/o/6vye5NiMs41h5+qwfg1razmGnYuTHv9h+7B4e6PmzyTHcqn9GDZvR82TXtvD2hvVx3H7M87xn/R5Y9Qx6tVf7vm0klYsJFXVtVV1QVVdcPrpz7msJ0mStK746TZJkqQOQ5IkSVLHyJCU5Hrga8B5SQ4kuXL63ZIkSZqtkV8BUFVvXo2OSJIkrSVebpMkSeowJEmSJHUYkiRJkjoMSZIkSR2GJEmSpA5DkiRJUochSZIkqcOQJEmS1GFIkiRJ6jAkSZIkdRiSJEmSOgxJkiRJHYYkSZKkDkOSJElShyFJkiSpw5AkSZLUYUiSJEnqMCRJkiR1GJIkSZI6DEmSJEkdhiRJkqQOQ5IkSVKHIUmSJKnDkCRJktRhSJIkSeowJEmSJHUYkiRJkjoMSZIkSR2GJEmSpA5DkiRJUochSZIkqcOQJEmS1GFIkiRJ6jAkSZIkdRiSJEmSOgxJkiRJHYYkSZKkDkOSJElShyFJkiSpw5AkSZLUYUiSJEnqMCRJkiR1GJIkSZI6xgpJSV6b5M4k+5JcPe1OSZIkzdrIkJTkOOBDwE5gB/DmJDum3TFJkqRZGueVpJcD+6rq7qr6PnADcOl0uyVJkjRbm8aocwZwz4LlA8ArFldKchVwVVt8MsmdR9+9obYmeajTj2fd99b11k+6PGzb45ZNsn45fR92LEb1c9jfD9tekq3AkuMyTnvD2h6zD0uuG1Z/JY7rOPs5zvpx2xizrDtXeu1MMg+Wcy5Peq5Nuv1RJmnnaJ8jxlg+MlfGnWNHcz4sp/44/Zr0uXJUv5ZzLo77d2M8fs6YLPdYLmdMJ32OGaefyy2fZOxXYqyGPO7+mzIFZ/cKxwlJY6mqa4FrV2p7oyTZXVUXrFZ7Go/jsvY4JmuT47L2OCZrz6zHZJzLbfcCZy1YPrOVSZIkbVjjhKSvA+cmOSfJ84E3ATdOt1uSJEmzNfJyW1U9neRtwE3AccB1VfWtqfdstFW7tKeJOC5rj2OyNjkua49jsvbMdExSVbNsX5IkaU3yG7clSZI6DEmSJEkd6zIkxZ9JmYkkZyX5cpJvJ/lWkne08lOT3JzkrnZ/SitPkj9t4/TNJOfPdg82riTHJfmXJF9oy+ckua0d+0+3D12QZHNb3tfWz8204xtYki1JdiW5I8neJD/tXJmtJL/VnrtuT3J9khOcK6svyXVJDia5fUHZxHMjyeWt/l1JLp9GX9ddSIo/kzJLTwO/XVU7gAuB32jH/mrglqo6F7ilLcNgjM5tt6uAD69+l48Z7wD2Llh+P3BNVb0IeBS4spVfCTzayq9p9TQdHwT+sapeDLyUwfg4V2YkyRnA24ELquolDD6I9CacK7PwMeC1i8ommhtJTgXew+DLrV8OvGc+WK2kdReS8GdSZqaq7q+qb7THTzB40j+DwfH/eKv2ceAN7fGlwCdq4FZgS5IfX91eb3xJzgReB3ykLQe4CNjVqiwek/mx2gW8Okt99a6WLcnJwM8DHwWoqu9X1WGcK7O2CfihJJuAFwD341xZdVX1VeCRRcWTzo3XADdX1SNV9ShwM88NXkdtPYak3s+knDGjvhyz2kvPLwNuA7ZV1f1t1QPAtvbYsVodfwL8DvBMWz4NOFxVT7flhcf9yJi09Y+1+lpZ5wCHgL9ol0E/kuREnCszU1X3Ah8AvssgHD0G7MG5slZMOjdWZc6sx5CkGUtyEvA54J1V9fjCdTX4Tgm/V2KVJHk9cLCq9sy6L3qWTcD5wIer6mXA9/j/yweAc2W1tUsxlzIIsNuBE5nCKw86emtpbqzHkOTPpMxQkuMZBKS/rKrPt+IH5y8NtPuDrdyxmr6fBX45yXcYXHq+iMF7Yba0Swrw7ON+ZEza+pOBh1ezw8eIA8CBqrqtLe9iEJqcK7PzS8B/VtWhqnoK+DyD+eNcWRsmnRurMmfWY0jyZ1JmpF2P/yiwt6r+eMGqG4H5TxZcDvztgvK3tE8nXAg8tuDlVK2AqnpXVZ1ZVXMM5sI/VdWvAl8GLmvVFo/J/Fhd1uqvif+xbSRV9QBwT5LzWtGrgW/jXJml7wIXJnlBey6bHxPnytow6dy4Cbg4ySntVcKLW9nKqqp1dwMuAf4D2A+8e9b9OVZuwCsZvAT6TeBf2+0SBtfpbwHuAr4EnNrqh8EnEfcD/87gUyUz34+NegNeBXyhPX4h8M/APuCzwOZWfkJb3tfWv3DW/d6oN+Angd1tvvwNcIpzZeZj8j7gDuB24JPAZufKTMbhegbvC3uKwauuVy5nbgC/1sZnH3DFNPrqz5JIkiR1rMfLbZIkSVNnSJIkSeowJEmSJHUYkiRJkjoMSZIkSR2GJEmSpA5DkiRJUsf/AbaK+AakXknmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAABlCAYAAABZcXdQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMl0lEQVR4nO3dfaxl1VnH8e9PBmgp7w5t5UUGUqQCCS3FAto0pCVAkRcTiQGRkopBjdXaVGlNiTjaGNRGsCnWYKEUpVCLDbaorRSq/CPYARveK4PyPjDDO9SWl/D4x1532GfPhntH5t5z7/D9JGfu2Wvts/ez19rr3mf22uecVBWSJEma9CPTDkCSJGkxMkmSJEkaYZIkSZI0wiRJkiRphEmSJEnSCJMkSZKkESZJkuYsycVJPjXtODZWktuSHD7tOCQtLcumHYAkbUpJLgYeqKqzZsqqav/pRSRpqfJKkqRFJYn/eZO0KJgkSdpAkp9M8q9JnmxTVcf3qpcnuTrJM0n+Lcme7TVJcm6StUmeTnJLkgNa3dZJPp3kviSPJPmrJG9sdYcneSDJx5M8DHwhyR1Jju3FsyzJuiQHteWvJHk4yVNJrkuyfys/AzgFODPJs0m+3srvSXJEL5bzkjzUHucl2XoQy8facaxJ8qFeHMckub0d+4NJfmf+ekHStJkkSZqQZEvg68C/AG8GfhO4NMm+bZVTgD8ClgPfBS5t5UcC7wV+AtgB+AXgsVZ3Tit/B/A2YDfg93u7fSuwM7AncAZwGXByr/4o4NGquqkt/zOwT4vvppkYquqC9vxPq2rbqjpu5BA/CRzaYjkQeDdwVq/+rS3+3YDTgfOT7NTqLgR+taq2Aw4Arh3ZvqTNhEmSpKFDgW2Bc6rq+aq6FriKl5OWf6yq66rqObqE47AkewAvANsBbwdSVXdU1ZokoUt8PlpVj1fVM8AfAyf19vkScHZVPVdVPwC+BByfZJtW/4t0iRMAVXVRVT3TYvgD4MAkO8zx+E4B/rCq1lbVOmAlcGqv/oVW/0JV/RPwLLBvr26/JNtX1RO9pE3SZsgkSdLQrsD9VfVSr+xeuisrAPfPFFbVs8DjwK4tmfoscD6wNskFSbYHdgG2AW5s03dPAt9o5TPWVdUPe9tdDdwBHNcSpePpEieSbJHknCR3J3kauKe9bPlGHN+9g2Pbtbf8WFW92Fv+X7qkEeDngWOAe9tU42Fz3KekJcgkSdLQQ8AeSfq/H34ceLA932OmMMm2dNNkDwFU1Weq6l3AfnTTa78LPAr8ANi/qnZsjx2qatve9mskjpkptxOA21viBN1VpROAI+imxVbMhPMq2xoe356DY3toltd0G676TlWdQDfNdyXwd3N5naSlySRJ0tANdFdPzkyyZft8oeOAy1v9MUnek2QrunuTrq+q+5P8VJJD2j1N3wd+CLzUrkj9NXBukjcDJNktyVGzxHE53X1Ov067itRsBzxHd7/TNnRTd32PAHu/ynYvA85KskuS5XT3Rv3tLLGQZKskpyTZoapeAJ6mmyaUtJkySZI0oaqep0uKPkB3FegvgQ9W1Z1tlS8BZ9NNs70L+KVWvj1dMvQE3RTWY8CftbqPA6uB69sU2bd4+T6fV4pjDfDvwE8DX+5VXdK2/yBwO3D94KUX0t039GSSK0c2/SlgFXAzcAvdjd9z/YDMU4F72jH8Gt39TZI2U6ma7cq0JEnS649XkiRJkkaYJEmSJI0wSZIkSRphkiRJkjRi1i+STHIRcCywtqoOmMtGly9fXitWrHiNoUmSJM2/G2+88dGq2mVYPpdv276Y7lN0L5nrzlasWMGqVavmHp0kSdKUJLl3rHzW6baquo7u81AkSZJeNzbZPUlJzkiyKsmqdevWbarNvvL+VgYIWRkS1j8gEz/XrzdT1lufDJ6vfPkxs7y+vG2rv36/rv3zcnl/m4PXzsQyE08/rpntTMS7srdtRvZDJrfXj73XBhuU99un107DY+zHtH5/g2Mf7meD2JmsG253ffyD9umvv75/Bu20vmzYDkw+xvp5/etH+np4LBPHPBPToL1GX5/BsTBot/6524tl4hwbnHuvuJ9h3yav/nPQLhMxDuKYaO9++2bDx/CcGx13g/2Nntf95Wy4PByzw3N82E/D4xqee/2Yh3012n+9NuzvY3guD2Pq91G/H4d92m/v0TiG/dA/vl75xDgfO1f6cQ77f+z4xtp62M6DdcfWH57jw3Nmok0G52J/HxPnwXCs9c+/YT/022+srwfnZ3/8T7TzK7X3RMiTY2Bi/4PXTLTf2N+twXFuMHZ759cG/d1v25WD8v6+R86zV/qdMfY3ZmKcj/1t7G1juL/J9pmuTZYkVdUFVXVwVR28yy4bTOtJkiQtKb67TZIkaYRJkiRJ0ohZk6Qkl9F9yeS+SR5Icvr8hyVJkjRds34EQFWdvBCBSJIkLSZOt0mSJI0wSZIkSRphkiRJkjTCJEmSJGmESZIkSdIIkyRJkqQRJkmSJEkjTJIkSZJGmCRJkiSNMEmSJEkaYZIkSZI0wiRJkiRphEmSJEnSCJMkSZKkESZJkiRJI0ySJEmSRpgkSZIkjTBJkiRJGmGSJEmSNMIkSZIkaYRJkiRJ0giTJEmSpBEmSZIkSSNMkiRJkkaYJEmSJI0wSZIkSRphkiRJkjTCJEmSJGmESZIkSdIIkyRJkqQRJkmSJEkjTJIkSZJGmCRJkiSNMEmSJEkaYZIkSZI0wiRJkiRphEmSJEnSCJMkSZKkESZJkiRJI0ySJEmSRpgkSZIkjTBJkiRJGmGSJEmSNGJOSVKSo5N8L8nqJJ+Y76AkSZKmbdYkKckWwPnAB4D9gJOT7DffgUmSJE3TXK4kvRtYXVX/XVXPA5cDJ8xvWJIkSdOVqnr1FZITgaOr6lfa8qnAIVX14cF6ZwBntMV9ge9t+nAnLAcened9aOPZL4uPfbI42S+Lj32y+CxUn+xZVbsMC5dtqq1X1QXABZtqe7NJsqqqDl6o/Wlu7JfFxz5ZnOyXxcc+WXym3SdzmW57ENijt7x7K5MkSdpszSVJ+g6wT5K9kmwFnAR8bX7DkiRJmq5Zp9uq6sUkHwa+CWwBXFRVt817ZLNbsKk9bRT7ZfGxTxYn+2XxsU8Wn6n2yaw3bkuSJL0e+YnbkiRJI0ySJEmSRizJJMmvSZmOJHsk+XaS25PcluQjrXznJFcnuav93KmVJ8lnWj/dnOSg6R7B5ivJFkn+M8lVbXmvJDe0tv9ye9MFSbZuy6tb/YqpBr4ZS7JjkiuS3JnkjiSHOVamK8lH2++uW5NcluQNjpWFl+SiJGuT3Nor2+ixkeS0tv5dSU6bj1iXXJLk16RM1YvAx6pqP+BQ4Dda238CuKaq9gGuacvQ9dE+7XEG8LmFD/l14yPAHb3lPwHOraq3AU8Ap7fy04EnWvm5bT3Nj78AvlFVbwcOpOsfx8qUJNkN+C3g4Ko6gO6NSCfhWJmGi4GjB2UbNTaS7AycDRxC980gZ88kVpvSkkuS8GtSpqaq1lTVTe35M3S/9Heja/8vttW+CPxce34CcEl1rgd2TPJjCxv15i/J7sDPAp9vywHeB1zRVhn2yUxfXQG8v62vTSjJDsB7gQsBqur5qnoSx8q0LQPemGQZsA2wBsfKgquq64DHB8UbOzaOAq6uqser6gngajZMvF6zpZgk7Qbc31t+oJVpAbVLz+8EbgDeUlVrWtXDwFvac/tqYZwHnAm81JZ/FHiyql5sy/12X98nrf6ptr42rb2AdcAX2jTo55O8CcfK1FTVg8CngfvokqOngBtxrCwWGzs2FmTMLMUkSVOWZFvg74Hfrqqn+3XVfaaEnyuxQJIcC6ytqhunHYsmLAMOAj5XVe8Evs/L0weAY2WhtamYE+gS2F2BNzEPVx702i2msbEUkyS/JmWKkmxJlyBdWlVfbcWPzEwNtJ9rW7l9Nf9+Bjg+yT10U8/vo7sXZsc2pQCT7b6+T1r9DsBjCxnw68QDwANVdUNbvoIuaXKsTM8RwP9U1bqqegH4Kt34cawsDhs7NhZkzCzFJMmvSZmSNh9/IXBHVf15r+prwMw7C04D/qFX/sH27oRDgad6l1O1CVTV71XV7lW1gm4sXFtVpwDfBk5sqw37ZKavTmzrL4r/sW1Oquph4P4k+7ai9wO341iZpvuAQ5Ns036XzfSJY2Vx2Nix8U3gyCQ7tauER7ayTauqltwDOAb4L+Bu4JPTjuf18gDeQ3cJ9Gbgu+1xDN08/TXAXcC3gJ3b+qF7J+LdwC107yqZ+nFsrg/gcOCq9nxv4D+A1cBXgK1b+Rva8upWv/e0495cH8A7gFVtvFwJ7ORYmXqfrATuBG4F/gbY2rEylX64jO6+sBforrqe/v8ZG8Avt/5ZDXxoPmL1a0kkSZJGLMXpNkmSpHlnkiRJkjTCJEmSJGmESZIkSdIIkyRJkqQRJkmSJEkjTJIkSZJG/B9UHL9R9aAZJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "def plot_samples(samples, state2color, title): \n",
    "    colors = [state2color[x] for x in samples]\n",
    "    x = np.arange(0, len(colors))\n",
    "    y = np.ones(len(colors))\n",
    "    plt.figure(figsize=(10,1))\n",
    "    plt.bar(x, y, color=colors, width=1)\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "transmat = np.array([[0.37, 0.63], \n",
    "                    [0.63, 0.37]])\n",
    "\n",
    "start_prob = np.array([1.0, 0.0])\n",
    "\n",
    "emission_probs = np.array([[0.15, 0.35, 0.35, 0.15], \n",
    "                           [0.4, 0.1, 0.1, 0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_prob \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probs\n",
    "\n",
    "# sample the model - X is the observed values \n",
    "# and Z is the \"hidden\" states \n",
    "X, Z = model.sample(1000)\n",
    "\n",
    "# we have to re-define state2color and obj2color as the hmm-learn \n",
    "# package just outputs numbers for the states \n",
    "state2color = {} \n",
    "state2color[0] = 'black'\n",
    "state2color[1] = 'white'\n",
    "plot_samples(Z, state2color, 'states')\n",
    "\n",
    "samples = [item for sublist in X for item in sublist]\n",
    "obj2color = {} \n",
    "obj2color[0] = 'red'\n",
    "obj2color[1] = 'green'\n",
    "obj2color[2] = 'blue'\n",
    "obj2color[3] = 'yellow'\n",
    "plot_samples(samples, obj2color, 'observations')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 (Expected) -1 point\n",
    "\n",
    "Generate 10000 samples using the defined hmm for generating DNA sequences. Learn the HMM in an unsupervised fashion similarly to what we did with the weather example i.e only use the observation samples not the \"hidden\" states. Constrast the original HMM to the HMM estimated from the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix\n",
      "Estimated model:\n",
      "[[0.86479711 0.13520289]\n",
      " [0.14190926 0.85809074]]\n",
      "Original model:\n",
      "[[0.85 0.15]\n",
      " [0.15 0.85]]\n",
      "Emission probabilities\n",
      "Estimated model\n",
      "[[0.39251353 0.11328235 0.10043382 0.39377031]\n",
      " [0.15416859 0.35081607 0.36593555 0.12907978]]\n",
      "Original model\n",
      "[[0.15 0.35 0.35 0.15]\n",
      " [0.4  0.1  0.1  0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "transmat = np.array([[0.85, 0.15], \n",
    "                    [0.15, 0.85]])\n",
    "\n",
    "start_prob = np.array([1.0, 0.0])\n",
    "\n",
    "emission_probs = np.array([[0.15, 0.35, 0.35, 0.15], \n",
    "                           [0.4, 0.1, 0.1, 0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_prob \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probs\n",
    "\n",
    "X, Z = model.sample(10000)\n",
    "# learn a new model \n",
    "estimated_model = hmm.MultinomialHMM(n_components=2, n_iter=1000000).fit(X)\n",
    "\n",
    "\n",
    "print(\"Transition matrix\")\n",
    "print(\"Estimated model:\")\n",
    "print(estimated_model.transmat_)\n",
    "print(\"Original model:\")\n",
    "print(model.transmat_)\n",
    "print(\"Emission probabilities\")\n",
    "print(\"Estimated model\")\n",
    "print(estimated_model.emissionprob_)\n",
    "print(\"Original model\")\n",
    "print(model.emissionprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 (Expected) -1 point\n",
    "\n",
    "Write a function called **classification_accuracy** that takes as input \n",
    "two arrays or lists of states and returns the number of states that are the same in both lists as a percentage. \n",
    "\n",
    "Consider the original sequences of states of the generated samples \n",
    "as ground truth. Then use the estimated model from the previous \n",
    "question to generate predicted states from the observation samples. \n",
    "That is the maximum likelihood sequence estimation problem. \n",
    "Note that the predicted states might be inverted compared to the original and you need to deal with that in your code (see the class notebook for details). Now compute the accuracy between the predicted \n",
    "sequence of states and the ground truth sequence of states. \n",
    "This is similar to the visual comparison of the original and predicted states in the provided notebook but using a quantified \n",
    "metric rather than a visualization. \n",
    "\n",
    "Now replace the transition model of the original HMM with a transition model that is all 0.5 i.e there is no transition information. Effectively this disregards any temporal dependenices and each time step is decided independently. In fact it corresponds to a Naive Bayes classifier with a single feature which is the nucleobase observation. \n",
    "\n",
    "What is the classification accuracy in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.6196\n",
      "Classification Accuracy 0.5: 0.6354\n"
     ]
    }
   ],
   "source": [
    "#   accuracy = #correct predictions / # instances\n",
    "#    TP + TN / TP + TN + FP + FN\n",
    "def flip(listNum):\n",
    "    flipped = []\n",
    "    for numbers in listNum:\n",
    "        if numbers == 0:\n",
    "            flipped.append(1)\n",
    "        elif numbers == 1:\n",
    "            flipped.append(0)\n",
    "    return flipped\n",
    "\n",
    "def classification_accuracy(first, second):\n",
    "    hold = flip(second)\n",
    "    first_length = len(first)\n",
    "    count2 = 0\n",
    "    countFlip = 0\n",
    "    for i in range(first_length):\n",
    "        if first[i] == second[i]:\n",
    "            count2 += 1\n",
    "        if first[i] == hold[i]:\n",
    "            countFlip += 1\n",
    "    accuracy = max((count2/first_length),(countFlip/first_length))\n",
    "    return(accuracy)        \n",
    "\n",
    "\n",
    "transmat = np.array([[0.85, 0.15], \n",
    "                    [0.15, 0.85]])\n",
    "\n",
    "start_prob = np.array([.5, 0.5])\n",
    "\n",
    "emission_probs = np.array([[0.15, 0.35, 0.35, 0.15], \n",
    "                           [0.4, 0.1, 0.1, 0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_prob \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probs\n",
    "\n",
    "X, Z = model.sample(10000)\n",
    "estimated_model = hmm.MultinomialHMM(n_components=2, n_iter=1000000).fit(X)\n",
    "estimateZ = estimated_model.predict(X)\n",
    "\n",
    "print(\"Classification Accuracy:\", classification_accuracy(Z,estimateZ))\n",
    "\n",
    "#classification accuracy using 0.5\n",
    "transmat = np.array([[0.5, 0.5], \n",
    "                    [0.5, 0.5]])\n",
    "\n",
    "start_prob = np.array([.5, 0.5])\n",
    "\n",
    "emission_probs = np.array([[0.15, 0.35, 0.35, 0.15], \n",
    "                           [0.4, 0.1, 0.1, 0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_prob \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probs\n",
    "\n",
    "X, Z = model.sample(10000)\n",
    "estimated_model = hmm.MultinomialHMM(n_components=2, n_iter=1000000).fit(X)\n",
    "estimateZ = estimated_model.predict(X)\n",
    "\n",
    "print(\"Classification Accuracy 0.5:\", classification_accuracy(Z,estimateZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 (Advanced) -1 point\n",
    "\n",
    "This question is a bit more open ended, will require some creativity and extra work. Consider the following problem: during your day your cell phone collects location data in terms of x,y coordinates. You do different activities such as going to university, eating, going to the gym. These activities take place in particular locations such as Restaurant A and Restaurant B or Gym A, Gym B and each particular location can be thought of as a two-dimensional Gaussian distribution of location points. If you consider the activity as the hidden state and the location as the observation you have a Hidden Markov Model. Because activities take place in multiple locations you can model this as a Gaussian Mixture Model (GMM). Each Gaussian will be multivariate 2D Gaussian distribution characterized by two means and and a 2 by 2 covariance matrix.\n",
    "\n",
    "Consider a hypothetical scenario with 3 activities (eat, study, exercise) and 3 locations (GMM components) for each activity. You will need to do some reading about how GMMs work. You can come up \n",
    "with reasonable estimates for the associated parameters. \n",
    "\n",
    "Basically the goal is the follow the format of the Markov Chain and HMM notebook and create appropriate visualizations using this problem.\n",
    "\n",
    "Visualize on a 2D plane using circles the different locations and corresponding mixture components\n",
    "Generate a dataset using a Hidden Markov Model of the problem\n",
    "Visualize the dataset on a 2D plane\n",
    "Show how you can learn the parameters of this HMM using https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GMMHMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9 (Basic) - 1 point\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this question is to get some familiarity with scikit-learn: https://scikit-learn.org/stable/\n",
    "\n",
    "Replicate movie review classification from the previous assignment using bernoulli Naive Bayes in sklearn. This is relatively straightforward you simply need to create appropriate binary feature matrix and labels. Report on the classification accuracy and confusion matrix for that problem using 3-fold cross-validation. \n",
    "You will need to consult the execllent sklearn documentation for details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX, y = load_iris(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\\nbnb = BernoulliNB()\\ny_pred = bnb.fit(X_train, y_train).predict(X_test)\\nprint(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#import file list\n",
    "directory_pos = 'pos'\n",
    "directory_neg = 'neg'\n",
    "\n",
    "review_files_pos = []\n",
    "review_files_neg = []\n",
    "total_files = []\n",
    "\n",
    "reviews = ['awful', 'bad', 'boring', 'dull', 'effective', 'enjoyable', 'great', 'hilarious']\n",
    "for subdir, dirs, files in os.walk(directory_pos):\n",
    "    for file in files:\n",
    "        review_files_pos.append(os.path.join(subdir, file))      \n",
    "\n",
    "for subdir, dirs, files in os.walk(directory_neg):\n",
    "    for file in files:\n",
    "        review_files_neg.append(os.path.join(subdir, file))    \n",
    "\n",
    "total_files = review_files_pos + review_files_neg   \n",
    "\n",
    "#get X\n",
    "vectorizer = CountVectorizer(vocabulary = reviews)\n",
    "X = vectorizer.fit_transform(total_files)\n",
    "\n",
    "#set up y\n",
    "def ycreate():\n",
    "    y=[]\n",
    "    for i in range(0,1000):\n",
    "        y.append(0)\n",
    "    for i in range(0,1000):\n",
    "        y.append(1)\n",
    "    y=np.array(y)\n",
    "    return y\n",
    "\n",
    "y = ycreate()\n",
    "\n",
    "#bernouilli naive bayes\n",
    "# https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "#3-fold cross validation\n",
    "kf = KFold(n_splits=3,random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_pred = bnb.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "\"\"\"\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10 (Expected) - 1 point \n",
    "\n",
    "The goal of this question is to give you some familiarity with having continuous features and comparing different classifiers. For this question use the breast cancer dataset from sklearn: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "\n",
    "Train and compare three classifiers using this dataset using 3-fold \n",
    "cross-validation to calculate the classification accuracy and classification report: \n",
    "\n",
    "1. The Gaussian Naive Bayes classifier (with default parameters) \n",
    "(from sklearn.naive_bayes import GaussianNB) \n",
    "2. Linear support vector machine (with default parameters) \n",
    "(from sklearn.svm import LinearSVC) \n",
    "3. Decision tree (with default parameters) \n",
    "(from sklearn.tree import DecisionTreeClassifier)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#gaussianNB\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#linear support vector machine\n",
    "\n",
    "\n",
    "#decision tree\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "cancer = load_breast_cancer()\n",
    "cross_val_score(clf, cancer.data, cancer.target, cv=10)\n",
    "\n",
    "#3-fold cross validation\n",
    "kf = KFold(n_splits=3,random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
